{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dataset() got an unexpected keyword argument 'channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m real_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/real/images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m fake_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/fake/images\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# contient des sous-dossiers\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m X, y, gen_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYCbCr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m f_model, g_models, h_model, label_enc \u001b[38;5;241m=\u001b[39m train_classifiers(X, y, gen_labels)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Test sur une image\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_dataset() got an unexpected keyword argument 'channels'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Param√®tres\n",
    "# ==========================\n",
    "BLOCK_SIZE = 8\n",
    "NUM_BLOCKS = 1000  # nombre de blocs √† retenir\n",
    "\n",
    "# ==========================\n",
    "# √âtape 1 : Extraction des Features de Bruit\n",
    "# ==========================\n",
    "def extract_noise_features(image_path, selected_channels=[\"Y\", \"Cb\", \"Cr\"]):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_ycc = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)  # OpenCV inverse Cb/Cr\n",
    "    \n",
    "    channel_map = {\n",
    "        \"Y\": img_ycc[:, :, 0],\n",
    "        \"Cb\": img_ycc[:, :, 2],  # Cb = channel 2\n",
    "        \"Cr\": img_ycc[:, :, 1]   # Cr = channel 1\n",
    "    }\n",
    "\n",
    "    features = []\n",
    "    # selected_channels = list(channels) if len(channels) <= 3 else [channels[i:i+2] for i in range(0, len(channels), 2)]\n",
    "\n",
    "    for ch in selected_channels:\n",
    "        if ch not in channel_map:\n",
    "            continue\n",
    "\n",
    "        Ic = channel_map[ch].astype(np.float32)\n",
    "        L4 = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=np.float32)\n",
    "        Fc = cv2.filter2D(Ic, -1, L4)\n",
    "\n",
    "        h, w = Fc.shape\n",
    "        blocks = [Fc[i:i+BLOCK_SIZE, j:j+BLOCK_SIZE].flatten()\n",
    "                  for i in range(0, h - BLOCK_SIZE + 1, BLOCK_SIZE)\n",
    "                  for j in range(0, w - BLOCK_SIZE + 1, BLOCK_SIZE)]\n",
    "\n",
    "        # S√©lection blocs √† faible variance et moyenne\n",
    "        block_vars = np.array([np.var(b) for b in blocks])\n",
    "        block_means = np.array([np.mean(b) for b in blocks])\n",
    "        idx_var = np.argsort(block_vars)[:NUM_BLOCKS]\n",
    "        idx_mean = np.argsort(block_means)[:NUM_BLOCKS]\n",
    "        idx_selected = np.intersect1d(idx_var, idx_mean)\n",
    "        selected_blocks = [blocks[i] for i in idx_selected]\n",
    "\n",
    "        # Matrice de corr√©lation\n",
    "        selected_blocks = np.stack(selected_blocks, axis=1)\n",
    "        Rc = np.corrcoef(selected_blocks)\n",
    "        tril_indices = np.tril_indices_from(Rc, k=-1)\n",
    "        SRc = Rc[tril_indices]\n",
    "\n",
    "        features.append(SRc)\n",
    "\n",
    "    return np.concatenate(features)\n",
    "\n",
    "# ==========================\n",
    "# Chargement du Dataset\n",
    "# ==========================\n",
    "def load_dataset(real_dir, fake_dir, selected_channels=[\"Y\", \"Cb\", \"Cr\"]):\n",
    "    X, y, generator_labels = [], [], []\n",
    "\n",
    "    # R√©elles\n",
    "    for img_path in tqdm(glob.glob(os.path.join(real_dir, \"*.jpg\"))):\n",
    "        X.append(extract_noise_features(img_path, selected_channels=selected_channels))\n",
    "        y.append(\"real\")\n",
    "        generator_labels.append(\"real\")\n",
    "\n",
    "    # Fakes par g√©n√©rateur\n",
    "    for gen_name in os.listdir(fake_dir):\n",
    "        gen_path = os.path.join(fake_dir, gen_name)\n",
    "        if not os.path.isdir(gen_path):\n",
    "            continue\n",
    "        for img_path in tqdm(glob.glob(os.path.join(gen_path, \"*.jpg\"))):\n",
    "            X.append(extract_noise_features(img_path, selected_channels=selected_channels))\n",
    "            y.append(\"fake\")\n",
    "            generator_labels.append(gen_name)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(generator_labels)\n",
    "\n",
    "# ==========================\n",
    "# Entra√Ænement du Pipeline Complet\n",
    "# ==========================\n",
    "\n",
    "# =============== TRAINING PIPELINE ===============\n",
    "def train_classifiers(X, y, gen_labels):\n",
    "    print(\"üîß Initialisation de l'entra√Ænement...\")\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    gen_indices = label_enc.fit_transform(gen_labels)\n",
    "    N = len(np.unique(gen_indices))\n",
    "\n",
    "    print(f\"üì¶ Nombre de g√©n√©rateurs diff√©rents : {N}\")\n",
    "    print(\"üîÑ Split des donn√©es pour le mod√®le f (g√©n√©rateur)...\")\n",
    "    X_train, X_test, gen_train, gen_test = train_test_split(X, gen_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è Entra√Ænement du mod√®le f (g√©n√©rateur)...\")\n",
    "    f_model = LogisticRegression(max_iter=1000)\n",
    "    f_model.fit(X_train, gen_train)\n",
    "    f_preds = f_model.predict(X_test)\n",
    "    f_probs = f_model.predict_proba(X_test)\n",
    "\n",
    "    print(\"\\nüìä M√©triques du mod√®le f (multi-class g√©n√©rateur):\")\n",
    "    print(classification_report(gen_test, f_preds, target_names=label_enc.classes_))\n",
    "    print(f\"üéØ Accuracy f_model: {accuracy_score(gen_test, f_preds):.4f}\")\n",
    "    print(f\"üî¢ Log loss f_model: {log_loss(gen_test, f_probs):.4f}\")\n",
    "\n",
    "    g_models = []\n",
    "    g_preds_all = []\n",
    "    print(\"\\nüèó Entra√Ænement des mod√®les g (par g√©n√©rateur)...\")\n",
    "    for i in range(N):\n",
    "        print(f\"  üîπ Mod√®le g pour le g√©n√©rateur '{label_enc.classes_[i]}'\")\n",
    "\n",
    "        gi_labels = np.array([(g == i or y[idx] == \"real\") for idx, g in enumerate(gen_indices)], dtype=int)\n",
    "        X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(X, gi_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        g_model = LogisticRegression(max_iter=1000)\n",
    "        g_model.fit(X_train_g, y_train_g)\n",
    "        preds = g_model.predict(X_test_g)\n",
    "        probs = g_model.predict_proba(X_test_g)[:, 1]\n",
    "\n",
    "        print(f\"    üéØ Accuracy: {accuracy_score(y_test_g, preds):.4f}\")\n",
    "        print(f\"    üî¢ Log loss: {log_loss(y_test_g, probs):.4f}\")\n",
    "        print(f\"    üßæ Report:\\n{classification_report(y_test_g, preds)}\")\n",
    "\n",
    "        g_models.append(g_model)\n",
    "        g_preds_all.append(g_model.predict_proba(X_test)[:, 1])  # tous √©valu√©s sur m√™me X_test que f_model\n",
    "\n",
    "    g_preds_all = np.stack(g_preds_all, axis=1)\n",
    "    final_input = np.concatenate([f_probs, g_preds_all], axis=1)\n",
    "\n",
    "    print(\"\\nüéØ Entra√Ænement du mod√®le h (final)...\")\n",
    "    mask = gen_indices != N-1  # Exclure le dernier si c‚Äôest ‚Äòreal‚Äô uniquement\n",
    "    h_labels = np.array([label != \"real\" for label in y])[mask]\n",
    "    h_model = LogisticRegression(max_iter=1000)\n",
    "    h_model.fit(final_input, h_labels[:final_input.shape[0]])\n",
    "    h_preds = h_model.predict(final_input)\n",
    "    h_probs = h_model.predict_proba(final_input)[:, 1]\n",
    "\n",
    "    print(\"\\nüìä M√©triques du mod√®le h (binaire FAKE vs REAL):\")\n",
    "    print(classification_report(h_labels[:final_input.shape[0]], h_preds))\n",
    "    print(f\"üéØ Accuracy h_model: {accuracy_score(h_labels[:final_input.shape[0]], h_preds):.4f}\")\n",
    "    print(f\"üî¢ Log loss h_model: {log_loss(h_labels[:final_input.shape[0]], h_probs):.4f}\")\n",
    "\n",
    "    print(\"‚úÖ Tous les mod√®les ont √©t√© entra√Æn√©s avec succ√®s.\")\n",
    "    return f_model, g_models, h_model, label_enc\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Inference sur une image\n",
    "# ==========================\n",
    "def predict_image(img_path, f_model, g_models, h_model, label_enc, selected_channels=[\"Y\", \"Cb\", \"Cr\"]):\n",
    "    x = extract_noise_features(img_path, selected_channels=selected_channels).reshape(1, -1)\n",
    "    f_out = f_model.predict_proba(x)\n",
    "    g_out = np.stack([g.predict_proba(x)[:, 1] for g in g_models], axis=1)\n",
    "    final_input = np.concatenate([f_out, g_out], axis=1)\n",
    "    final_score = h_model.predict_proba(final_input)[:, 1]\n",
    "    return final_score\n",
    "\n",
    "# ==========================\n",
    "# Exemple d'utilisation\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    real_dir = \"/path/to/real/images\"\n",
    "    fake_dir = \"/path/to/fake/images\"  # contient des sous-dossiers\n",
    "    \n",
    "    X, y, gen_labels = load_dataset(real_dir, fake_dir, channels=\"YCbCr\")\n",
    "    f_model, g_models, h_model, label_enc = train_classifiers(X, y, gen_labels)\n",
    "\n",
    "    # Test sur une image\n",
    "    test_img = \"/path/to/test/image.jpg\"\n",
    "    score = predict_image(test_img, f_model, g_models, h_model, label_enc)\n",
    "    print(f\"Score final (probabilit√© d'√™tre FAKE): {score[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'C', 'r']\n",
      "True\n",
      "['Y', 'C', 'r']\n"
     ]
    }
   ],
   "source": [
    "channels = \"YCbCr\"\n",
    "selected_channels = list(channels) if len(channels) <= 3 else [channels[i:i+2] for i in range(0, len(channels), 2)]\n",
    "print(selected_channels)\n",
    "print(len(channels) <= 3)\n",
    "print(list(channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting fake image features:   0%|          | 19/76000 [00:00<16:06, 78.59it/s]/home/sitcharn/anaconda3/envs/paper/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/sitcharn/anaconda3/envs/paper/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "Extracting fake image features:   0%|          | 99/76000 [00:01<21:33, 58.67it/s]\n",
      "Extracting real image features:   0%|          | 99/118287 [00:01<32:53, 59.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total features extraites: 200\n",
      "‚ùå Images ignor√©es: 0\n",
      "üìä Total analys√©: 200\n",
      "üì¶ G√©n√©rateurs d√©tect√©s: {'ADM', 'VQDM', 'Midjourney', 'real', 'stable_diffusion_v_1_5', 'wukong', 'stable_diffusion_v_1_4'}\n",
      "X shape: (200, 14850)\n",
      "y shape: (200,)\n",
      "üîß Initialisation de l'entra√Ænement...\n",
      "\n",
      "üìà R√©partition des classes :\n",
      "  - fake: 100 (50.00%)\n",
      "  - real: 100 (50.00%)\n",
      "\n",
      "üì¶ Nombre de g√©n√©rateurs diff√©rents : 7\n",
      "üîÑ Split des donn√©es pour le mod√®le f (g√©n√©rateur)...\n",
      "y_test:  ['fake' 'fake' 'fake' 'real' 'real' 'real' 'fake' 'real' 'real' 'fake'\n",
      " 'fake' 'real' 'real' 'fake' 'real' 'real' 'fake' 'real' 'fake' 'fake'\n",
      " 'real' 'fake' 'real' 'fake' 'fake' 'fake' 'fake' 'fake' 'real' 'real'\n",
      " 'fake' 'fake' 'fake' 'fake' 'real' 'real' 'real' 'real' 'real' 'fake']\n",
      "üèãÔ∏è‚Äç‚ôÇÔ∏è Entra√Ænement du mod√®le f (g√©n√©rateur)...\n",
      "\n",
      "üìä M√©triques du mod√®le f (multi-class g√©n√©rateur):\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                   ADM       0.00      0.00      0.00         1\n",
      "            Midjourney       0.00      0.00      0.00         5\n",
      "                  VQDM       0.00      0.00      0.00         3\n",
      "                  real       0.53      0.95      0.68        19\n",
      "stable_diffusion_v_1_4       0.00      0.00      0.00         3\n",
      "stable_diffusion_v_1_5       0.00      0.00      0.00         5\n",
      "                wukong       0.00      0.00      0.00         4\n",
      "\n",
      "              accuracy                           0.45        40\n",
      "             macro avg       0.08      0.14      0.10        40\n",
      "          weighted avg       0.25      0.45      0.32        40\n",
      "\n",
      "üéØ Accuracy f_model: 0.4500\n",
      "üî¢ Log loss f_model: 2.2679\n",
      "\n",
      "üèó Entra√Ænement des mod√®les g (par g√©n√©rateur)...\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'ADM'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sitcharn/anaconda3/envs/paper/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/sitcharn/anaconda3/envs/paper/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/sitcharn/anaconda3/envs/paper/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üéØ Accuracy: 0.7250\n",
      "    üî¢ Log loss: 0.8543\n",
      "    üß† AUC: 0.7050\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.60      0.69        20\n",
      "           1       0.68      0.85      0.76        20\n",
      "\n",
      "    accuracy                           0.72        40\n",
      "   macro avg       0.74      0.72      0.72        40\n",
      "weighted avg       0.74      0.72      0.72        40\n",
      "\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'Midjourney'\n",
      "    üéØ Accuracy: 0.7000\n",
      "    üî¢ Log loss: 0.7884\n",
      "    üß† AUC: 0.7214\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.38      0.50        16\n",
      "           1       0.69      0.92      0.79        24\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.72      0.65      0.64        40\n",
      "weighted avg       0.71      0.70      0.67        40\n",
      "\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'VQDM'\n",
      "    üéØ Accuracy: 0.6250\n",
      "    üî¢ Log loss: 0.9634\n",
      "    üß† AUC: 0.6869\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.39      0.48        18\n",
      "           1       0.62      0.82      0.71        22\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.63      0.60      0.59        40\n",
      "weighted avg       0.63      0.62      0.61        40\n",
      "\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'real'\n",
      "    üéØ Accuracy: 0.6500\n",
      "    üî¢ Log loss: 0.8511\n",
      "    üß† AUC: 0.7293\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.52      0.61        21\n",
      "           1       0.60      0.79      0.68        19\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.67      0.66      0.65        40\n",
      "weighted avg       0.67      0.65      0.64        40\n",
      "\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'stable_diffusion_v_1_4'\n",
      "    üéØ Accuracy: 0.6750\n",
      "    üî¢ Log loss: 0.7384\n",
      "    üß† AUC: 0.7576\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.44      0.55        18\n",
      "           1       0.66      0.86      0.75        22\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.69      0.65      0.65        40\n",
      "weighted avg       0.69      0.68      0.66        40\n",
      "\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'stable_diffusion_v_1_5'\n",
      "    üéØ Accuracy: 0.5500\n",
      "    üî¢ Log loss: 0.9532\n",
      "    üß† AUC: 0.6380\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.19      0.25        16\n",
      "           1       0.59      0.79      0.68        24\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.48      0.49      0.46        40\n",
      "weighted avg       0.51      0.55      0.51        40\n",
      "\n",
      "  üîπ Mod√®le g pour le g√©n√©rateur 'wukong'\n",
      "    üéØ Accuracy: 0.6000\n",
      "    üî¢ Log loss: 1.1903\n",
      "    üß† AUC: 0.5703\n",
      "    üßæ Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.29      0.38        17\n",
      "           1       0.61      0.83      0.70        23\n",
      "\n",
      "    accuracy                           0.60        40\n",
      "   macro avg       0.58      0.56      0.54        40\n",
      "weighted avg       0.59      0.60      0.57        40\n",
      "\n",
      "\n",
      "üéØ Entra√Ænement du mod√®le h (final FAKE vs REAL)...\n",
      "\n",
      "üéØ Entra√Ænement du mod√®le h (final FAKE vs REAL)...\n",
      "\n",
      "üìä M√©triques du mod√®le h (binaire FAKE vs REAL):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63        19\n",
      "           1       0.67      0.57      0.62        21\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.63      0.63      0.62        40\n",
      "weighted avg       0.63      0.62      0.62        40\n",
      "\n",
      "üéØ Accuracy h_model: 0.6250\n",
      "üî¢ Log loss h_model: 0.5905\n",
      "üìà AUC h_model: 0.7243\n",
      "‚úÖ Tous les mod√®les ont √©t√© entra√Æn√©s avec succ√®s.\n",
      "\n",
      "‚úÖ Entra√Ænement termin√©\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets.arrow_dataset import Dataset as ArrowDataset\n",
    "from datasets import concatenate_datasets\n",
    "from glob import glob\n",
    "\n",
    "# =============== CONFIG ===============\n",
    "FAKE_ARROW_DIR = \"/medias/db/ImagingSecurity_misc/sitcharn/paper_reproduction/cache/datasets/nebula___df-arrow/default/0.0.0/93117d58649bcf660f80fecf2122fac1f59d0453\"\n",
    "REAL_DIR = \"/medias/db/ImagingSecurity_misc/Collaborations/Hermes deepfake challenge/data/defacto/COCO/train2017\"\n",
    "BLOCK_SIZE = 8\n",
    "NUM_BLOCKS = 100\n",
    "CHANNELS = [\"Y\", \"Cb\", \"Cr\"]\n",
    "MAX_REAL_IMAGES = 100\n",
    "FEATURE_SIZE = int((NUM_BLOCKS * (NUM_BLOCKS - 1) / 2) * len(CHANNELS))  # Corr√©lation triangulaire\n",
    "\n",
    "# =============== FEATURE EXTRACTION ===============\n",
    "def extract_noise_features(image_bytes, selected_channels=CHANNELS):\n",
    "    try:\n",
    "        img_array = np.frombuffer(image_bytes, np.uint8)\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise ValueError(\"Image non d√©codable\")\n",
    "\n",
    "        img_ycc = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "        channel_map = {\n",
    "            \"Y\": img_ycc[:, :, 0],\n",
    "            \"Cb\": img_ycc[:, :, 2],\n",
    "            \"Cr\": img_ycc[:, :, 1]\n",
    "        }\n",
    "\n",
    "        features = []\n",
    "        for ch in selected_channels:\n",
    "            Ic = channel_map[ch].astype(np.float32)\n",
    "            L4 = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=np.float32)\n",
    "            Fc = cv2.filter2D(Ic, -1, L4)\n",
    "\n",
    "            h, w = Fc.shape\n",
    "            blocks = [Fc[i:i+BLOCK_SIZE, j:j+BLOCK_SIZE].flatten()\n",
    "                      for i in range(0, h - BLOCK_SIZE + 1, BLOCK_SIZE)\n",
    "                      for j in range(0, w - BLOCK_SIZE + 1, BLOCK_SIZE)]\n",
    "\n",
    "            if len(blocks) == 0:\n",
    "                continue\n",
    "\n",
    "            while len(blocks) < NUM_BLOCKS:\n",
    "                blocks += blocks  # r√©plication\n",
    "            blocks = blocks[:NUM_BLOCKS]\n",
    "\n",
    "            selected_blocks = np.stack(blocks, axis=1)\n",
    "            Rc = np.corrcoef(selected_blocks)\n",
    "            if np.isnan(Rc).any():\n",
    "                continue\n",
    "\n",
    "            tril_indices = np.tril_indices_from(Rc, k=-1)\n",
    "            SRc = Rc[tril_indices]\n",
    "            features.append(SRc)\n",
    "\n",
    "        if not features:\n",
    "            raise ValueError(\"Aucune feature extraite\")\n",
    "\n",
    "        full_feat = np.concatenate(features)\n",
    "\n",
    "        if full_feat.shape[0] != FEATURE_SIZE:\n",
    "            pad_width = FEATURE_SIZE - full_feat.shape[0]\n",
    "            full_feat = np.pad(full_feat, (0, pad_width), mode='constant')\n",
    "\n",
    "        return full_feat\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur dans l'image : {e}\")\n",
    "\n",
    "# =============== TRAINING PIPELINE ===============\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, roc_auc_score\n",
    "\n",
    "# =============== TRAINING PIPELINE ===============\n",
    "# =============== TRAINING PIPELINE ===============\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\n",
    "\n",
    "def train_classifiers(X, y, gen_labels):\n",
    "    print(\"üîß Initialisation de l'entra√Ænement...\")\n",
    "\n",
    "    # Affichage du ratio fake/real\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    counts_dict = dict(zip(unique, counts))\n",
    "    total = sum(counts)\n",
    "    print(\"\\nüìà R√©partition des classes :\")\n",
    "    for cls in sorted(counts_dict):\n",
    "        pct = 100 * counts_dict[cls] / total\n",
    "        print(f\"  - {cls}: {counts_dict[cls]} ({pct:.2f}%)\")\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    gen_indices = label_enc.fit_transform(gen_labels)\n",
    "    N = len(np.unique(gen_indices))\n",
    "\n",
    "    print(f\"\\nüì¶ Nombre de g√©n√©rateurs diff√©rents : {N}\")\n",
    "    print(\"üîÑ Split des donn√©es pour le mod√®le f (g√©n√©rateur)...\")\n",
    "    X_train, X_test, gen_train, gen_test, y_train, y_test, gen_labels_train, gen_labels_test = train_test_split(\n",
    "        X, gen_indices, y, gen_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"y_test: \", y_test)\n",
    "\n",
    "    print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è Entra√Ænement du mod√®le f (g√©n√©rateur)...\")\n",
    "    f_model = LogisticRegression(max_iter=1000)\n",
    "    f_model.fit(X_train, gen_train)\n",
    "    f_preds = f_model.predict(X_test)\n",
    "    f_probs = f_model.predict_proba(X_test)\n",
    "\n",
    "    print(\"\\nüìä M√©triques du mod√®le f (multi-class g√©n√©rateur):\")\n",
    "    print(classification_report(gen_test, f_preds, target_names=label_enc.classes_))\n",
    "    print(f\"üéØ Accuracy f_model: {accuracy_score(gen_test, f_preds):.4f}\")\n",
    "    print(f\"üî¢ Log loss f_model: {log_loss(gen_test, f_probs):.4f}\")\n",
    "\n",
    "    g_models = []\n",
    "    g_preds_all = []\n",
    "    print(\"\\nüèó Entra√Ænement des mod√®les g (par g√©n√©rateur)...\")\n",
    "    for i in range(N):\n",
    "        print(f\"  üîπ Mod√®le g pour le g√©n√©rateur '{label_enc.classes_[i]}'\")\n",
    "\n",
    "        gi_labels = np.array([(g == i or y[idx] == \"real\") for idx, g in enumerate(gen_indices)], dtype=int)\n",
    "        X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(X, gi_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        g_model = LogisticRegression(max_iter=1000)\n",
    "        g_model.fit(X_train_g, y_train_g)\n",
    "        preds = g_model.predict(X_test_g)\n",
    "        probs = g_model.predict_proba(X_test_g)[:, 1]\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test_g, probs)\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "\n",
    "        print(f\"    üéØ Accuracy: {accuracy_score(y_test_g, preds):.4f}\")\n",
    "        print(f\"    üî¢ Log loss: {log_loss(y_test_g, probs):.4f}\")\n",
    "        print(f\"    üß† AUC: {auc:.4f}\")\n",
    "        print(f\"    üßæ Report:\\n{classification_report(y_test_g, preds)}\")\n",
    "\n",
    "        g_models.append(g_model)\n",
    "        g_preds_all.append(g_model.predict_proba(X_test)[:, 1])  # tous √©valu√©s sur m√™me X_test que f_model\n",
    "\n",
    "    g_preds_all = np.stack(g_preds_all, axis=1)\n",
    "    final_input = np.concatenate([f_probs, g_preds_all], axis=1)\n",
    "\n",
    "    print(\"\\nüéØ Entra√Ænement du mod√®le h (final FAKE vs REAL)...\")\n",
    "\n",
    "    print(\"\\nüéØ Entra√Ænement du mod√®le h (final FAKE vs REAL)...\")\n",
    "\n",
    "    # Labels binaires : 1 = fake, 0 = real\n",
    "    h_labels = (np.array(y_test) == \"fake\").astype(int)\n",
    "\n",
    "    # V√©rifier qu'on a bien les deux classes\n",
    "    if len(np.unique(h_labels)) < 2:\n",
    "        raise ValueError(\"‚ö†Ô∏è Pas assez de classes (fake/real) pour entra√Æner h_model.\")\n",
    "\n",
    "    # Entra√Ænement\n",
    "    h_model = LogisticRegression(max_iter=1000)\n",
    "    h_model.fit(final_input, h_labels)\n",
    "\n",
    "    # Pr√©diction\n",
    "    h_preds = h_model.predict(final_input)\n",
    "    h_probs = h_model.predict_proba(final_input)[:, 1]\n",
    "\n",
    "    # Affichage des m√©triques\n",
    "    print(\"\\nüìä M√©triques du mod√®le h (binaire FAKE vs REAL):\")\n",
    "    print(classification_report(h_labels, h_preds))\n",
    "    print(f\"üéØ Accuracy h_model: {accuracy_score(h_labels, h_preds):.4f}\")\n",
    "    print(f\"üî¢ Log loss h_model: {log_loss(h_labels, h_probs):.4f}\")\n",
    "    print(f\"üìà AUC h_model: {roc_auc_score(h_labels, h_probs):.4f}\")\n",
    "\n",
    "    print(\"‚úÖ Tous les mod√®les ont √©t√© entra√Æn√©s avec succ√®s.\")\n",
    "\n",
    "    return f_model, g_models, h_model, label_enc\n",
    "\n",
    "\n",
    "# =============== MAIN PIPELINE ===============\n",
    "if __name__ == \"__main__\":\n",
    "    arrow_files = sorted([\n",
    "        os.path.join(FAKE_ARROW_DIR, f) for f in os.listdir(FAKE_ARROW_DIR)\n",
    "        if f.startswith(\"df-arrow-test\") and f.endswith(\".arrow\")\n",
    "    ])\n",
    "    fake_dataset = concatenate_datasets([ArrowDataset.from_file(f) for f in arrow_files])\n",
    "\n",
    "    X, y, generator_labels = [], [], []\n",
    "    success_count, fail_count = 0, 0\n",
    "\n",
    "    for sample in tqdm(fake_dataset, desc=\"Extracting fake image features\"):\n",
    "        try:\n",
    "            img_bytes = sample[\"image\"]\n",
    "            path = sample[\"image_path\"]\n",
    "            gen_name = path.split(\"/\")[0]\n",
    "            feat = extract_noise_features(img_bytes)\n",
    "            X.append(feat)\n",
    "            y.append(\"fake\")\n",
    "            generator_labels.append(gen_name)\n",
    "            success_count += 1\n",
    "        except Exception:\n",
    "            fail_count += 1\n",
    "        if success_count >= MAX_REAL_IMAGES:\n",
    "            break\n",
    "    \n",
    "    real_image_paths = sorted(\n",
    "        glob(os.path.join(REAL_DIR, \"**\", \"*.jpg\"), recursive=True)\n",
    "        + glob(os.path.join(REAL_DIR, \"**\", \"*.png\"), recursive=True)\n",
    "    )\n",
    "\n",
    "    real_count = 0\n",
    "    for path in tqdm(real_image_paths, desc=\"Extracting real image features\"):\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                img_bytes = f.read()\n",
    "                feat = extract_noise_features(img_bytes)\n",
    "\n",
    "                if feat is None or len(feat.shape) != 1 or (len(X) > 0 and feat.shape[0] != X[0].shape[0]):\n",
    "                    raise ValueError(\"Vecteur de features invalide ou incoh√©rent\")\n",
    "\n",
    "                X.append(feat)\n",
    "                y.append(\"real\")\n",
    "                generator_labels.append(\"real\")\n",
    "                success_count += 1\n",
    "                real_count += 1\n",
    "                if real_count >= MAX_REAL_IMAGES:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pour image r√©elle {os.path.basename(path)}: {e}\")\n",
    "            fail_count += 1\n",
    "\n",
    "    print(f\"\\n‚úÖ Total features extraites: {success_count}\")\n",
    "    print(f\"‚ùå Images ignor√©es: {fail_count}\")\n",
    "    print(f\"üìä Total analys√©: {success_count + fail_count}\")\n",
    "    print(f\"üì¶ G√©n√©rateurs d√©tect√©s: {set(generator_labels)}\")\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    generator_labels = np.array(generator_labels)\n",
    "\n",
    "    f_model, g_models, h_model, label_enc = train_classifiers(X, y, generator_labels)\n",
    "    print(\"\\n‚úÖ Entra√Ænement termin√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
